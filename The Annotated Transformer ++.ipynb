{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Annotated Transformer ++\n",
    "\n",
    "The idea of this notebook is to make it easier even for non-researchers to understand the omnipresent transformer model!\n",
    "\n",
    "In this notebook you'll learn:\n",
    "\n",
    "âœ… What are transformers exactly? <br/>\n",
    "âœ… How to train them? <br/>\n",
    "âœ… How to use them? (machine translation example) <br/>\n",
    "\n",
    "After you complete this one you'll have a much better understanding of transformers!\n",
    "\n",
    "So, let's start!\n",
    "\n",
    "---\n",
    "\n",
    "## What the heck are transformers and how they came to be?\n",
    "\n",
    "Transformers were originally proposed by Vaswani et al. in a seminal paper called [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "You probably heard of transformers one way or another. GPT-3 and BERT to name a few well known ones ðŸ¦„. The main idea is that they showed that you don't have to use recurrent or convolutional layers and that simple architecture coupled with attention is super powerful. It gave the benefit of much better long-range dependencies modeling and the architecture itself is very parallelizable (ðŸ’»ðŸ’»ðŸ’») which leads to higher compute efficiency!\n",
    "\n",
    "Here is how their beautifully simple architecture looks like:\n",
    "\n",
    "<img src=\"data/readme_pics/transformer_architecture.PNG\" alt=\"transformer architecture\" align=\"center\" style=\"width: 350px;\"/> <br/>\n",
    "\n",
    "---\n",
    "\n",
    "That was everything you need to know for now! Let's get into the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always like to structure my imports into Python's native libs,\n",
    "# stuff I installed via conda/pip and local file imports (we don't have those here)\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some constants to make stuff a bit cleaner\n",
    "\n",
    "# Architecture related constants taken from the paper\n",
    "BASELINE_MODEL_NUMBER_OF_LAYERS = 6\n",
    "BASELINE_MODEL_DIMENSION = 512\n",
    "BASELINE_MODEL_NUMBER_OF_HEADS = 8\n",
    "BASELINE_MODEL_DROPOUT_PROB = 0.1\n",
    "BASELINE_MODEL_LABEL_SMOOTHING_VALUE = 0.1\n",
    "\n",
    "CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints') # semi-trained models during training will be dumped here\n",
    "BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries') # location where trained models are located\n",
    "DATA_DIR_PATH = os.path.join(os.getcwd(), 'data') # training data will be stored here\n",
    "\n",
    "os.makedirs(CHECKPOINTS_PATH, exist_ok=True)\n",
    "os.makedirs(BINARIES_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_DIR_PATH, exist_ok=True)\n",
    "\n",
    "# Special token symbols used later in the data section\n",
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the model (transformer)\n",
    "\n",
    "There are 2 really important parts of any deep learning pipeline:\n",
    "1. Model ðŸ¦„\n",
    "2. Data\n",
    "\n",
    "Let's start with the model (ðŸ¦„) first and understand how the transformer is architectured.\n",
    "\n",
    "I'll take a top-down approach here.\n",
    "\n",
    "We should first understand how all of the modules fit together (**level 0**) and then we'll start digging deeper into the nitty-gritty details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, src_vocab_size, trg_vocab_size, number_of_heads, number_of_layers, dropout_probability, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeds source/target token ids into embedding vectors\n",
    "        self.src_embedding = Embedding(src_vocab_size, model_dimension)\n",
    "        self.trg_embedding = Embedding(trg_vocab_size, model_dimension)\n",
    "\n",
    "        # Adds positional information to source/target token's embedding vector\n",
    "        # (otherwise we'd lose the positional information which is important in human languages)\n",
    "        self.src_pos_embedding = PositionalEncoding(model_dimension, dropout_probability)\n",
    "        self.trg_pos_embedding = PositionalEncoding(model_dimension, dropout_probability)\n",
    "\n",
    "        # All of these will get deep-copied multiple times internally\n",
    "        mha = MultiHeadedAttention(model_dimension, number_of_heads, dropout_probability, log_attention_weights)\n",
    "        pwn = PositionwiseFeedForwardNet(model_dimension, dropout_probability)\n",
    "        encoder_layer = EncoderLayer(model_dimension, dropout_probability, mha, pwn)\n",
    "        decoder_layer = DecoderLayer(model_dimension, dropout_probability, mha, pwn)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder = Encoder(encoder_layer, number_of_layers)\n",
    "        self.decoder = Decoder(decoder_layer, number_of_layers)\n",
    "\n",
    "        # Converts final target token representations into log probabilities vectors of the target vocab size\n",
    "        self.decoder_generator = DecoderGenerator(model_dimension, trg_vocab_size)\n",
    "        \n",
    "        self.init_params()\n",
    "\n",
    "    # This part wasn't mentioned in the paper, but it's super important!\n",
    "    def init_params(self):\n",
    "        # I tested both PyTorch's default initialization and this, and xavier has tremendous impact! I didn't expect\n",
    "        # a model's perf, with normalization layers, to be so much dependent on the choice of weight initialization.\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src_token_ids_batch, trg_token_ids_batch, src_mask, trg_mask):\n",
    "        src_representations_batch = self.encode(src_token_ids_batch, src_mask)\n",
    "        trg_log_probs = self.decode(trg_token_ids_batch, src_representations_batch, trg_mask, src_mask)\n",
    "        return trg_log_probs\n",
    "\n",
    "    # Modularized into encode/decode functions for optimizing the decoding/translation process (we'll get to it later)\n",
    "    def encode(self, src_token_ids_batch, src_mask):\n",
    "        src_embeddings_batch = self.src_embedding(src_token_ids_batch)  # get embedding vectors for src token ids\n",
    "        src_embeddings_batch = self.src_pos_embedding(src_embeddings_batch)  # add positional embedding\n",
    "        src_representations_batch = self.encoder(src_embeddings_batch, src_mask)  # forward pass through the encoder\n",
    "\n",
    "        return src_representations_batch\n",
    "\n",
    "    def decode(self, trg_token_ids_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        trg_embeddings_batch = self.trg_embedding(trg_token_ids_batch)  # get embedding vectors for trg token ids\n",
    "        trg_embeddings_batch = self.trg_pos_embedding(trg_embeddings_batch)  # add positional embedding\n",
    "        \n",
    "        # Shape (B, T, D), where B - batch size, T - longest target token-sequence length and D - model dimension\n",
    "        trg_representations_batch = self.decoder(trg_embeddings_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # After this line we'll have a shape (B, T, V), where V - target vocab size, decoder generator does a simple\n",
    "        # linear projection followed by log softmax\n",
    "        trg_log_probs = self.decoder_generator(trg_representations_batch)\n",
    "\n",
    "        # Reshape into (B*T, V) as that's a suitable format for passing it into KL div loss\n",
    "        trg_log_probs = trg_log_probs.reshape(-1, trg_log_probs.shape[-1])\n",
    "\n",
    "        return trg_log_probs  # the reason I use log here is that PyTorch's nn.KLDivLoss expects log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder\n",
    "\n",
    "Stay with me here! \n",
    "\n",
    "These initial parts will probably be the most confusing as you're not yet familiar with terminology and I'll reference many things yet to be defined.\n",
    "\n",
    "Get used to it. Whether you're reading research papers or code things can't always get linearized. <br/>\n",
    "Think of it as a graph which can't get converted into [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (direct acyclic graph) because we have circular dependencies!\n",
    "\n",
    "Let's go **1 level** deeper - into how encoder and decoder are structured.\n",
    "\n",
    "<img src=\"data/readme_pics/deeper_inception.jpg\" alt=\"we need to go deeper\" align=\"left\" style=\"width: 350px;\"/> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High level overview of the functionality:\n",
    "\n",
    "1. The encoder takes as the input a batch of source sequences whose tokens have already been embedded.\n",
    "2. It then does 6 iterations (6 layers for the base transformer) of mixing those via attention.\n",
    "3. The final output will get later consumed by the decoder. That's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        assert isinstance(encoder_layer, EncoderLayer), f'Expected EncoderLayer got {type(encoder_layer)}.'\n",
    "\n",
    "        # Get a list of 'number_of_layers' independent encoder layers\n",
    "        self.encoder_layers = get_clones(encoder_layer, number_of_layers)  \n",
    "        self.norm = nn.LayerNorm(encoder_layer.model_dimension)\n",
    "\n",
    "    def forward(self, src_embeddings_batch, src_mask):\n",
    "        # Just update the naming so as to reflect the semantics of what this var will become (the initial encoder layer\n",
    "        # has embedding vectors as input but later layers have richer token representations)\n",
    "        src_representations_batch = src_embeddings_batch\n",
    "\n",
    "        # Forward pass through the encoder stack\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # src_mask's role is to mask/ignore padded token representations in the multi-headed self-attention module\n",
    "            src_representations_batch = encoder_layer(src_representations_batch, src_mask)\n",
    "\n",
    "        # Not mentioned explicitly in the paper \n",
    "        # (a consequence of using LayerNorm before instead of after the SublayerLogic module)\n",
    "        return self.norm(src_representations_batch)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n",
    "        super().__init__()\n",
    "        num_of_sublayers_encoder = 2\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_encoder)\n",
    "\n",
    "        self.multi_headed_attention = multi_headed_attention\n",
    "        self.pointwise_net = pointwise_net\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, src_representations_batch, src_mask):\n",
    "        # Define anonymous (lambda) function which only takes src_representations_batch (srb) as input,\n",
    "        # this way we have a uniform interface for the sublayer logic.\n",
    "        encoder_self_attention = lambda srb: self.multi_headed_attention(query=srb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by point-wise feed forward net sublayer\n",
    "        # SublayerLogic takes as the input both the data and the logic it should execute (attention/feedforward)\n",
    "        src_representations_batch = self.sublayers[0](src_representations_batch, encoder_self_attention)\n",
    "        src_representations_batch = self.sublayers[1](src_representations_batch, self.pointwise_net)\n",
    "\n",
    "        return src_representations_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder has a similar structure to encoder.\n",
    "\n",
    "1. It again starts with a batch of target sequences whose tokens have already been embedded.\n",
    "2. It then does 6 iterations (6 layers for the base transformer) of mixing those via attention (this time also attending to source token representations)\n",
    "3. The final output i.e. target token representations go into the DecoderGenerator module which will convert those\n",
    "into log probabilities\n",
    "\n",
    "A really important difference is that the decoder uses **causal masking** so as to prevent tokens from looking into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, decoder_layer, number_of_layers):\n",
    "        super().__init__()\n",
    "        assert isinstance(decoder_layer, DecoderLayer), f'Expected DecoderLayer got {type(decoder_layer)}.'\n",
    "\n",
    "        self.decoder_layers = get_clones(decoder_layer, number_of_layers)\n",
    "        self.norm = nn.LayerNorm(decoder_layer.model_dimension)\n",
    "\n",
    "    def forward(self, trg_embeddings_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        # Just update the naming so as to reflect the semantics of what this var will become\n",
    "        trg_representations_batch = trg_embeddings_batch\n",
    "\n",
    "        # Forward pass through the decoder stack\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # Target mask masks pad tokens as well as future tokens (current target token can't look forward)\n",
    "            trg_representations_batch = decoder_layer(trg_representations_batch, src_representations_batch, trg_mask, src_mask)\n",
    "\n",
    "        # Not mentioned explicitly in the paper \n",
    "        # (a consequence of using LayerNorm before instead of after the SublayerLogic module)\n",
    "        return self.norm(trg_representations_batch)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, multi_headed_attention, pointwise_net):\n",
    "        super().__init__()\n",
    "        num_of_sublayers_decoder = 3\n",
    "        self.sublayers = get_clones(SublayerLogic(model_dimension, dropout_probability), num_of_sublayers_decoder)\n",
    "\n",
    "        self.trg_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n",
    "        self.src_multi_headed_attention = copy.deepcopy(multi_headed_attention)\n",
    "        self.pointwise_net = pointwise_net\n",
    "\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, trg_representations_batch, src_representations_batch, trg_mask, src_mask):\n",
    "        # Define anonymous (lambda) function which only takes trg_representations_batch (trb - funny name I know)\n",
    "        # as input - this way we have a uniform interface for the sublayer logic.\n",
    "        # The inputs which are not passed into lambdas are \"cached\" here that's why the thing works.\n",
    "        srb = src_representations_batch  # simple/short alias\n",
    "        decoder_trg_self_attention = lambda trb: self.trg_multi_headed_attention(query=trb, key=trb, value=trb, mask=trg_mask)\n",
    "        decoder_src_attention = lambda trb: self.src_multi_headed_attention(query=trb, key=srb, value=srb, mask=src_mask)\n",
    "\n",
    "        # Self-attention MHA sublayer followed by a source-attending MHA and point-wise feed forward net sublayer\n",
    "        trg_representations_batch = self.sublayers[0](trg_representations_batch, decoder_trg_self_attention)\n",
    "        trg_representations_batch = self.sublayers[1](trg_representations_batch, decoder_src_attention)\n",
    "        trg_representations_batch = self.sublayers[2](trg_representations_batch, self.pointwise_net)\n",
    "\n",
    "        return trg_representations_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final **level 1** module is the DecoderGenerator (level 0 was the topmost Transformer class).\n",
    "\n",
    "This module does 2 things:\n",
    "1. Projects the final decoder token representations of dimension D into V dimensions (target vocabulary size)\n",
    "2. Applies a log softmax ([PyTorch's KL div loss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) expects log probabilities again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGenerator(nn.Module):\n",
    "    def __init__(self, model_dimension, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(model_dimension, vocab_size)\n",
    "\n",
    "        # -1 stands for apply the log-softmax along the last dimension i.e. over the vocab dimension as the output from\n",
    "        # the linear layer has shape (B, T, V), B - batch size, T - max target token-sequence, V - target vocab size\n",
    "        \n",
    "        # again using log softmax as PyTorch's nn.KLDivLoss expects log probabilities (just a technical detail)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, trg_representations_batch):\n",
    "        # Project from D (model dimension) into V (target vocab size) and apply the log softmax along V dimension\n",
    "        return self.log_softmax(self.linear(trg_representations_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 modules\n",
    "\n",
    "Phew, tap yourself on the back and take a deep breath as we're diving into level 2 modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the original paper had LayerNorm AFTER the residual connection and addition operation\n",
    "# multiple experiments I found showed that it's more effective to do it BEFORE, how did they figure out which one is\n",
    "# better? Experiments! There is a similar thing in DCGAN and elsewhere.\n",
    "class SublayerLogic(nn.Module):\n",
    "    def __init__(self, model_dimension, dropout_probability):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(model_dimension)\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "    def forward(self, representations_batch, sublayer_module):\n",
    "        # Residual connection between input and sublayer output, details: Page 7, Chapter 5.4 \"Regularization\",\n",
    "        return representations_batch + self.dropout(sublayer_module(self.norm(representations_batch)))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForwardNet(nn.Module):\n",
    "    \"\"\"\n",
    "        It's position-wise because this feed forward net will be independently applied to every token's representation.\n",
    "\n",
    "        Representations batch is of the shape (batch size, max token sequence length, model dimension).\n",
    "        This net will basically be applied independently to every token's representation (you can think of it as if\n",
    "        there was a nested for-loop going over the batch size and max token sequence length dimensions\n",
    "        and applied this net to token representations. PyTorch does this auto-magically behind the scenes.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dimension, dropout_probability, width_mult=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(model_dimension, width_mult * model_dimension)\n",
    "        self.linear2 = nn.Linear(width_mult * model_dimension, model_dimension)\n",
    "\n",
    "        # This dropout layer is not explicitly mentioned in the paper but it's common to use to avoid over-fitting\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, representations_batch):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(representations_batch))))\n",
    "\n",
    "\n",
    "#\n",
    "# Input modules\n",
    "#\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dimension):\n",
    "        super().__init__()\n",
    "        self.embeddings_table = nn.Embedding(vocab_size, model_dimension)\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, token_ids_batch):\n",
    "        assert token_ids_batch.ndim == 2, f'Expected: (batch size, max token sequence length), got {token_ids_batch.shape}'\n",
    "\n",
    "        # token_ids_batch has shape (B, S/T), where B - batch size, S/T max src/trg token-sequence length\n",
    "        # Final shape will be (B, S/T, D) where D is the model dimension, every token id has associated vector\n",
    "        embeddings = self.embeddings_table(token_ids_batch)\n",
    "\n",
    "        # (stated in the paper) multiply the embedding weights by the square root of model dimension\n",
    "        # Page 5, Chapter 3.4 \"Embeddings and Softmax\"\n",
    "        return embeddings * math.sqrt(self.model_dimension)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_probability, expected_max_sequence_length=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_probability)\n",
    "\n",
    "        # (stated in the paper) Use sine functions whose frequencies form a geometric progression as position encodings,\n",
    "        # (learning encodings will also work so feel free to change it!). Page 6, Chapter 3.5 \"Positional Encoding\"\n",
    "        position_id = torch.arange(0, expected_max_sequence_length).unsqueeze(1)\n",
    "        frequencies = torch.pow(10000., -torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n",
    "\n",
    "        # Checkout playground.py for visualization of how these look like (it's super simple don't get scared)\n",
    "        positional_encodings_table = torch.zeros(expected_max_sequence_length, model_dimension)\n",
    "        positional_encodings_table[:, 0::2] = torch.sin(position_id * frequencies)  # sine on even positions\n",
    "        positional_encodings_table[:, 1::2] = torch.cos(position_id * frequencies)  # cosine on odd positions\n",
    "\n",
    "        # Register buffer because we want to save the positional encodings table inside state_dict even though\n",
    "        # these are not trainable (not model's parameters) so they otherwise would be excluded from the state_dict\n",
    "        self.register_buffer('positional_encodings_table', positional_encodings_table)\n",
    "\n",
    "    def forward(self, embeddings_batch):\n",
    "        assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], \\\n",
    "            f'Expected (batch size, max token sequence length, model dimension) got {embeddings_batch.shape}'\n",
    "\n",
    "        # embedding_batch's shape = (B, S/T, D), where S/T max src/trg token-sequence length, D - model dimension\n",
    "        # So here we get (S/T, D) shape which will get broad-casted to (B, S/T, D) when we try and add it to embeddings\n",
    "        positional_encodings = self.positional_encodings_table[:embeddings_batch.shape[1]]\n",
    "\n",
    "        # (stated in the paper) Applying dropout to the sum of positional encodings and token embeddings\n",
    "        # Page 7, Chapter 5.4 \"Regularization\"\n",
    "        return self.dropout(embeddings_batch + positional_encodings)\n",
    "\n",
    "\n",
    "#\n",
    "# Helper model functions\n",
    "#\n",
    "\n",
    "\n",
    "def get_clones(module, num_of_deep_copies):\n",
    "    # Create deep copies so that we can tweak each module's weights independently\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_of_deep_copies)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The attention part of \"Attention Is All You Need\"\n",
    "\n",
    "And last but definitely not least - the multi headed attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        This module already exists in PyTorch. The reason I implemented it here from scratch is that\n",
    "        PyTorch implementation is super complicated as they made it as generic/robust as possible whereas\n",
    "        on the other hand I only want to support a limited use-case.\n",
    "\n",
    "        Also this is arguable the most important architectural component in the Transformer model.\n",
    "\n",
    "        Additional note:\n",
    "        This is conceptually super easy stuff. It's just that matrix implementation makes things a bit less intuitive.\n",
    "        If you take your time and go through the code and figure out all of the dimensions + write stuff down on paper\n",
    "        you'll understand everything. Also do check out this amazing blog for conceptual understanding:\n",
    "\n",
    "        https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "        Optimization notes:\n",
    "\n",
    "        qkv_nets could be replaced by Parameter(torch.empty(3 * model_dimension, model_dimension)) and one more matrix\n",
    "        for bias, which would make the implementation a bit more optimized. For the sake of easier understanding though,\n",
    "        I'm doing it like this - using 3 \"feed forward nets\" (without activation/identity hence the quotation marks).\n",
    "        Conceptually both implementations are the same.\n",
    "\n",
    "        PyTorch's query/key/value are of different shape namely (max token sequence length, batch size, model dimension)\n",
    "        whereas I'm using (batch size, max token sequence length, model dimension) because it's easier to understand\n",
    "        and consistent with computer vision apps (batch dimension is always first followed by the number of channels (C)\n",
    "        and image's spatial dimensions height (H) and width (W) -> (B, C, H, W).\n",
    "\n",
    "        This has an important optimization implication, they can reshape their matrix into (B*NH, S/T, HD)\n",
    "        (where B - batch size, S/T - max src/trg sequence length, NH - number of heads, HD - head dimension)\n",
    "        in a single step and I can only get to (B, NH, S/T, HD) in single step\n",
    "        (I could call contiguous() followed by view but that's expensive as it would incur additional matrix copy)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension, number_of_heads, dropout_probability, log_attention_weights):\n",
    "        super().__init__()\n",
    "        assert model_dimension % number_of_heads == 0, f'Model dimension must be divisible by the number of heads.'\n",
    "\n",
    "        self.head_dimension = int(model_dimension / number_of_heads)\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        self.qkv_nets = get_clones(nn.Linear(model_dimension, model_dimension), 3)  # identity activation hence \"nets\"\n",
    "        self.out_projection_net = nn.Linear(model_dimension, model_dimension)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(p=dropout_probability)  # no pun intended, not explicitly mentioned in paper\n",
    "        self.softmax = nn.Softmax(dim=-1)  # -1 stands for apply the softmax along the last dimension\n",
    "\n",
    "        self.log_attention_weights = log_attention_weights  # should we log attention weights\n",
    "        self.attention_weights = None  # for visualization purposes, I cache the weights here (translation_script.py)\n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        # Step 1: Scaled dot-product attention, Page 4, Chapter 3.2.1 \"Scaled Dot-Product Attention\"\n",
    "        # Notation: B - batch size, S/T max src/trg token-sequence length, NH - number of heads, HD - head dimension\n",
    "        # query/key/value shape = (B, NH, S/T, HD), scores shape = (B, NH, S, S), (B, NH, T, T) or (B, NH, T, S)\n",
    "        # scores have different shapes as MHA is used in 3 contexts, self attention for src/trg and source attending MHA\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dimension)\n",
    "\n",
    "        # Step 2: Optionally mask tokens whose representations we want to ignore by setting a big negative number\n",
    "        # to locations corresponding to those tokens (force softmax to output 0 probability on those locations).\n",
    "        # mask shape = (B, 1, 1, S) or (B, 1, T, T) will get broad-casted (copied) as needed to match scores shape\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == torch.tensor(False), float(\"-inf\"))\n",
    "\n",
    "        # Step 3: Calculate the attention weights - how much should we attend to surrounding token representations\n",
    "        attention_weights = self.softmax(scores)\n",
    "\n",
    "        # Step 4: Not defined in the original paper apply dropout to attention weights as well\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "\n",
    "        # Step 5: based on attention weights calculate new token representations\n",
    "        # attention_weights shape = (B, NH, S, S)/(B, NH, T, T) or (B, NH, T, S), value shape = (B, NH, S/T, HD)\n",
    "        # Final shape (B, NH, S, HD) for source MHAs or (B, NH, T, HD) target MHAs (again MHAs are used in 3 contexts)\n",
    "        intermediate_token_representations = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return intermediate_token_representations, attention_weights  # attention weights for visualization purposes\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Step 1: Input linear projection\n",
    "        # Notation: B - batch size, NH - number of heads, S/T - max src/trg token-sequence length, HD - head dimension\n",
    "        # Shape goes from (B, S/T, NH*HD) over (B, S/T, NH, HD) to (B, NH, S/T, HD) (NH*HD=D where D is model dimension)\n",
    "        query, key, value = [net(x).view(batch_size, -1, self.number_of_heads, self.head_dimension).transpose(1, 2)\n",
    "                             for net, x in zip(self.qkv_nets, (query, key, value))]\n",
    "\n",
    "        # Step 2: Apply attention - compare query with key and use that to combine values (see the function for details)\n",
    "        intermediate_token_representations, attention_weights = self.attention(query, key, value, mask)\n",
    "\n",
    "        # Potentially, for visualization purposes, log the attention weights, turn off during training though!\n",
    "        # I had memory problems when I leave this on by default\n",
    "        if self.log_attention_weights:\n",
    "            self.attention_weights = attention_weights\n",
    "\n",
    "        # Step 3: Reshape from (B, NH, S/T, HD) over (B, S/T, NH, HD) (via transpose) into (B, S/T, NHxHD) which is\n",
    "        # the same shape as in the beginning of this forward function i.e. input to MHA (multi-head attention) module\n",
    "        reshaped = intermediate_token_representations.transpose(1, 2).reshape(batch_size, -1, self.number_of_heads * self.head_dimension)\n",
    "\n",
    "        # Step 4: Output linear projection\n",
    "        token_representations = self.out_projection_net(reshaped)\n",
    "\n",
    "        return token_representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with me\n",
    "\n",
    "I share lots of useful (I hope so at least!) content on LinkedIn and YouTube. <br/>\n",
    "So feel free to connect with me there:\n",
    "1. [My LinkedIn profile](https://www.linkedin.com/in/aleksagordic)\n",
    "2. [My YouTube channel - The AI Epiphany](https://www.youtube.com/c/TheAiEpiphany)\n",
    "\n",
    "Also do drop me a message if you found this useful or if you think I could've done something better! <br/>\n",
    "I always like getting some feedback on the work I do.\n",
    "\n",
    "# Additional resources\n",
    "\n",
    "I highly recommend [this blog by Jay Alammar](https://jalammar.github.io/illustrated-transformer/) to everyone learning about transformers.\n",
    "\n",
    "\n",
    "Also do check out my [YouTube video](https://www.youtube.com/watch?v=bvBK-coXf9I) on how I structured my transformers learning journey!\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=bvBK-coXf9I\" target=\"_blank\"><img src=\"https://img.youtube.com/vi/bvBK-coXf9I/0.jpg\" \n",
    "alt=\"learning about transformers\" width=\"480\" align=\"left\" height=\"360\" border=\"10\" /></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
